{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lL3oXB8oCY2j"
      },
      "source": [
        "\n",
        "### Step 1: Installing Necessary Libraries\n",
        "\n",
        "Before we begin, it's essential to install the required packages. Here's a breakdown of what each package does:\n",
        "\n",
        "- `openai`: The official Python client for the OpenAI API. It facilitates interactions with OpenAI's GPT models.\n",
        "- `langchain`: A library possibly providing tools for chaining language models.\n",
        "- `tiktoken`: An OpenAI library used to count the number of tokens in a string without making an API call.\n",
        "- `unstructured[slack]` and `unstructured_inference`: Libraries that handle unstructured data, with a specific extension for Slack.\n",
        "- `singlestoredb`: The official Python client for SingleStore DB to interact with SingleStore databases.\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "AvI0GStRMIO9",
        "outputId": "57ef9850-c351-48f3-8d71-914ae52af32a",
        "ExecuteTime": {
          "end_time": "2023-09-06T23:57:45.260082Z",
          "start_time": "2023-09-06T23:57:37.116894Z"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting openai\n",
            "  Downloading openai-0.28.1-py3-none-any.whl (76 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.0/77.0 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting langchain\n",
            "  Downloading langchain-0.0.326-py3-none-any.whl (1.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tiktoken\n",
            "  Downloading tiktoken-0.5.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m29.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests>=2.20 in /usr/local/lib/python3.10/dist-packages (from openai) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from openai) (3.8.6)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.22)\n",
            "Requirement already satisfied: anyio<4.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.7.1)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain)\n",
            "  Downloading dataclasses_json-0.6.1-py3-none-any.whl (27 kB)\n",
            "Collecting jsonpatch<2.0,>=1.33 (from langchain)\n",
            "  Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
            "Collecting langsmith<0.1.0,>=0.0.52 (from langchain)\n",
            "  Downloading langsmith-0.0.54-py3-none-any.whl (43 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.7/43.7 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.23.5)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.10.13)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (8.2.3)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2023.6.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (3.3.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.3.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<4.0->langchain) (3.4)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<4.0->langchain) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<4.0->langchain) (1.1.3)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain)\n",
            "  Downloading marshmallow-3.20.1-py3-none-any.whl (49 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain)\n",
            "  Downloading jsonpointer-2.4-py2.py3-none-any.whl (7.8 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (4.5.0)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (2023.7.22)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.0)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.10/dist-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json<0.7,>=0.5.7->langchain) (23.2)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Installing collected packages: mypy-extensions, marshmallow, jsonpointer, typing-inspect, tiktoken, langsmith, jsonpatch, openai, dataclasses-json, langchain\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "llmx 0.0.15a0 requires cohere, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed dataclasses-json-0.6.1 jsonpatch-1.33 jsonpointer-2.4 langchain-0.0.326 langsmith-0.0.54 marshmallow-3.20.1 mypy-extensions-1.0.0 openai-0.28.1 tiktoken-0.5.1 typing-inspect-0.9.0\n",
            "Collecting unstructured[slack]\n",
            "  Downloading unstructured-0.10.27-py3-none-any.whl (1.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting unstructured_inference\n",
            "  Downloading unstructured_inference-0.7.10-py3-none-any.whl (61 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.8/61.8 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: chardet in /usr/local/lib/python3.10/dist-packages (from unstructured[slack]) (5.2.0)\n",
            "Collecting filetype (from unstructured[slack])\n",
            "  Downloading filetype-1.2.0-py2.py3-none-any.whl (19 kB)\n",
            "Collecting python-magic (from unstructured[slack])\n",
            "  Downloading python_magic-0.4.27-py2.py3-none-any.whl (13 kB)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from unstructured[slack]) (4.9.3)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from unstructured[slack]) (3.8.1)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.10/dist-packages (from unstructured[slack]) (0.9.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from unstructured[slack]) (2.31.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from unstructured[slack]) (4.11.2)\n",
            "Collecting emoji (from unstructured[slack])\n",
            "  Downloading emoji-2.8.0-py2.py3-none-any.whl (358 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m358.9/358.9 kB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: dataclasses-json in /usr/local/lib/python3.10/dist-packages (from unstructured[slack]) (0.6.1)\n",
            "Collecting python-iso639 (from unstructured[slack])\n",
            "  Downloading python_iso639-2023.6.15-py3-none-any.whl (275 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m275.1/275.1 kB\u001b[0m \u001b[31m17.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting langdetect (from unstructured[slack])\n",
            "  Downloading langdetect-1.0.9.tar.gz (981 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m25.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from unstructured[slack]) (1.23.5)\n",
            "Collecting rapidfuzz (from unstructured[slack])\n",
            "  Downloading rapidfuzz-3.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m39.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting backoff (from unstructured[slack])\n",
            "  Downloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from unstructured[slack]) (4.5.0)\n",
            "Collecting slack-sdk (from unstructured[slack])\n",
            "  Downloading slack_sdk-3.23.0-py2.py3-none-any.whl (281 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m281.0/281.0 kB\u001b[0m \u001b[31m24.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting layoutparser[layoutmodels,tesseract] (from unstructured_inference)\n",
            "  Downloading layoutparser-0.3.4-py3-none-any.whl (19.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.2/19.2 MB\u001b[0m \u001b[31m43.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting python-multipart (from unstructured_inference)\n",
            "  Downloading python_multipart-0.0.6-py3-none-any.whl (45 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.7/45.7 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting huggingface-hub (from unstructured_inference)\n",
            "  Downloading huggingface_hub-0.18.0-py3-none-any.whl (301 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.0/302.0 kB\u001b[0m \u001b[31m26.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: opencv-python!=4.7.0.68 in /usr/local/lib/python3.10/dist-packages (from unstructured_inference) (4.8.0.76)\n",
            "Collecting onnx (from unstructured_inference)\n",
            "  Downloading onnx-1.15.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (15.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.7/15.7 MB\u001b[0m \u001b[31m41.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting onnxruntime<1.16 (from unstructured_inference)\n",
            "  Downloading onnxruntime-1.15.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.9/5.9 MB\u001b[0m \u001b[31m69.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting transformers>=4.25.1 (from unstructured_inference)\n",
            "  Downloading transformers-4.34.1-py3-none-any.whl (7.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.7/7.7 MB\u001b[0m \u001b[31m104.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting coloredlogs (from onnxruntime<1.16->unstructured_inference)\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: flatbuffers in /usr/local/lib/python3.10/dist-packages (from onnxruntime<1.16->unstructured_inference) (23.5.26)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from onnxruntime<1.16->unstructured_inference) (23.2)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from onnxruntime<1.16->unstructured_inference) (3.20.3)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from onnxruntime<1.16->unstructured_inference) (1.12)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers>=4.25.1->unstructured_inference) (3.12.4)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.25.1->unstructured_inference) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.25.1->unstructured_inference) (2023.6.3)\n",
            "Collecting tokenizers<0.15,>=0.14 (from transformers>=4.25.1->unstructured_inference)\n",
            "  Downloading tokenizers-0.14.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m97.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting safetensors>=0.3.1 (from transformers>=4.25.1->unstructured_inference)\n",
            "  Downloading safetensors-0.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m77.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.25.1->unstructured_inference) (4.66.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->unstructured_inference) (2023.6.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->unstructured[slack]) (2.5)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json->unstructured[slack]) (3.20.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json->unstructured[slack]) (0.9.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from langdetect->unstructured[slack]) (1.16.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from layoutparser[layoutmodels,tesseract]->unstructured_inference) (1.11.3)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from layoutparser[layoutmodels,tesseract]->unstructured_inference) (1.5.3)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from layoutparser[layoutmodels,tesseract]->unstructured_inference) (9.4.0)\n",
            "Collecting iopath (from layoutparser[layoutmodels,tesseract]->unstructured_inference)\n",
            "  Downloading iopath-0.1.10.tar.gz (42 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.2/42.2 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pdfplumber (from layoutparser[layoutmodels,tesseract]->unstructured_inference)\n",
            "  Downloading pdfplumber-0.10.3-py3-none-any.whl (48 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.0/49.0 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pdf2image (from layoutparser[layoutmodels,tesseract]->unstructured_inference)\n",
            "  Downloading pdf2image-1.16.3-py3-none-any.whl (11 kB)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from layoutparser[layoutmodels,tesseract]->unstructured_inference) (2.1.0+cu118)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from layoutparser[layoutmodels,tesseract]->unstructured_inference) (0.16.0+cu118)\n",
            "Collecting effdet (from layoutparser[layoutmodels,tesseract]->unstructured_inference)\n",
            "  Downloading effdet-0.4.1-py3-none-any.whl (112 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m112.5/112.5 kB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pytesseract (from layoutparser[layoutmodels,tesseract]->unstructured_inference)\n",
            "  Downloading pytesseract-0.3.10-py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->unstructured[slack]) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->unstructured[slack]) (1.3.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->unstructured[slack]) (3.3.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->unstructured[slack]) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->unstructured[slack]) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->unstructured[slack]) (2023.7.22)\n",
            "Collecting huggingface-hub (from unstructured_inference)\n",
            "  Downloading huggingface_hub-0.17.3-py3-none-any.whl (295 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.0/295.0 kB\u001b[0m \u001b[31m27.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json->unstructured[slack]) (1.0.0)\n",
            "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime<1.16->unstructured_inference)\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting timm>=0.9.2 (from effdet->layoutparser[layoutmodels,tesseract]->unstructured_inference)\n",
            "  Downloading timm-0.9.8-py3-none-any.whl (2.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m84.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pycocotools>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from effdet->layoutparser[layoutmodels,tesseract]->unstructured_inference) (2.0.7)\n",
            "Collecting omegaconf>=2.0 (from effdet->layoutparser[layoutmodels,tesseract]->unstructured_inference)\n",
            "  Downloading omegaconf-2.3.0-py3-none-any.whl (79 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.5/79.5 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->layoutparser[layoutmodels,tesseract]->unstructured_inference) (3.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->layoutparser[layoutmodels,tesseract]->unstructured_inference) (3.1.2)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch->layoutparser[layoutmodels,tesseract]->unstructured_inference) (2.1.0)\n",
            "Collecting portalocker (from iopath->layoutparser[layoutmodels,tesseract]->unstructured_inference)\n",
            "  Downloading portalocker-2.8.2-py3-none-any.whl (17 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->layoutparser[layoutmodels,tesseract]->unstructured_inference) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->layoutparser[layoutmodels,tesseract]->unstructured_inference) (2023.3.post1)\n",
            "Collecting pdfminer.six==20221105 (from pdfplumber->layoutparser[layoutmodels,tesseract]->unstructured_inference)\n",
            "  Downloading pdfminer.six-20221105-py3-none-any.whl (5.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m97.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pypdfium2>=4.18.0 (from pdfplumber->layoutparser[layoutmodels,tesseract]->unstructured_inference)\n",
            "  Downloading pypdfium2-4.22.0-py3-none-manylinux_2_17_x86_64.whl (2.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m89.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.10/dist-packages (from pdfminer.six==20221105->pdfplumber->layoutparser[layoutmodels,tesseract]->unstructured_inference) (41.0.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->onnxruntime<1.16->unstructured_inference) (1.3.0)\n",
            "Collecting antlr4-python3-runtime==4.9.* (from omegaconf>=2.0->effdet->layoutparser[layoutmodels,tesseract]->unstructured_inference)\n",
            "  Downloading antlr4-python3-runtime-4.9.3.tar.gz (117 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.0/117.0 kB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: matplotlib>=2.1.0 in /usr/local/lib/python3.10/dist-packages (from pycocotools>=2.0.2->effdet->layoutparser[layoutmodels,tesseract]->unstructured_inference) (3.7.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->layoutparser[layoutmodels,tesseract]->unstructured_inference) (2.1.3)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography>=36.0.0->pdfminer.six==20221105->pdfplumber->layoutparser[layoutmodels,tesseract]->unstructured_inference) (1.16.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->effdet->layoutparser[layoutmodels,tesseract]->unstructured_inference) (1.1.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->effdet->layoutparser[layoutmodels,tesseract]->unstructured_inference) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->effdet->layoutparser[layoutmodels,tesseract]->unstructured_inference) (4.43.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->effdet->layoutparser[layoutmodels,tesseract]->unstructured_inference) (1.4.5)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->effdet->layoutparser[layoutmodels,tesseract]->unstructured_inference) (3.1.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six==20221105->pdfplumber->layoutparser[layoutmodels,tesseract]->unstructured_inference) (2.21)\n",
            "Building wheels for collected packages: langdetect, iopath, antlr4-python3-runtime\n",
            "  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993224 sha256=8fcaf1c012dc46589374955db1cfd6f2005ac52e4ee4972002f80bc84c14b8e3\n",
            "  Stored in directory: /root/.cache/pip/wheels/95/03/7d/59ea870c70ce4e5a370638b5462a7711ab78fba2f655d05106\n",
            "  Building wheel for iopath (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for iopath: filename=iopath-0.1.10-py3-none-any.whl size=31530 sha256=4d3407679f84ec5fb9d7213a837a50ddd392926f39432dc4c77694b7f613c19b\n",
            "  Stored in directory: /root/.cache/pip/wheels/9a/a3/b6/ac0fcd1b4ed5cfeb3db92e6a0e476cfd48ed0df92b91080c1d\n",
            "  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.9.3-py3-none-any.whl size=144554 sha256=6f196b6f41c0f1502ed61193b8c8c5bb0e8e64b7afbffcda004d77c7432bd7cd\n",
            "  Stored in directory: /root/.cache/pip/wheels/12/93/dd/1f6a127edc45659556564c5730f6d4e300888f4bca2d4c5a88\n",
            "Successfully built langdetect iopath antlr4-python3-runtime\n",
            "Installing collected packages: filetype, antlr4-python3-runtime, slack-sdk, safetensors, rapidfuzz, python-multipart, python-magic, python-iso639, pytesseract, pypdfium2, portalocker, pdf2image, onnx, omegaconf, langdetect, humanfriendly, emoji, backoff, iopath, huggingface-hub, coloredlogs, unstructured, tokenizers, pdfminer.six, onnxruntime, transformers, timm, pdfplumber, layoutparser, effdet, unstructured_inference\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "lida 0.0.10 requires fastapi, which is not installed.\n",
            "lida 0.0.10 requires kaleido, which is not installed.\n",
            "lida 0.0.10 requires uvicorn, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed antlr4-python3-runtime-4.9.3 backoff-2.2.1 coloredlogs-15.0.1 effdet-0.4.1 emoji-2.8.0 filetype-1.2.0 huggingface-hub-0.17.3 humanfriendly-10.0 iopath-0.1.10 langdetect-1.0.9 layoutparser-0.3.4 omegaconf-2.3.0 onnx-1.15.0 onnxruntime-1.15.1 pdf2image-1.16.3 pdfminer.six-20221105 pdfplumber-0.10.3 portalocker-2.8.2 pypdfium2-4.22.0 pytesseract-0.3.10 python-iso639-2023.6.15 python-magic-0.4.27 python-multipart-0.0.6 rapidfuzz-3.4.0 safetensors-0.4.0 slack-sdk-3.23.0 timm-0.9.8 tokenizers-0.14.1 transformers-4.34.1 unstructured-0.10.27 unstructured_inference-0.7.10\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "pydevd_plugins"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting singlestoredb\n",
            "  Downloading singlestoredb-0.9.3-cp36-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (277 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/277.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━\u001b[0m \u001b[32m204.8/277.8 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m277.8/277.8 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: PyJWT in /usr/lib/python3/dist-packages (from singlestoredb) (2.3.0)\n",
            "Requirement already satisfied: build in /usr/local/lib/python3.10/dist-packages (from singlestoredb) (1.0.3)\n",
            "Collecting parsimonious (from singlestoredb)\n",
            "  Downloading parsimonious-0.10.0-py3-none-any.whl (48 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.4/48.4 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from singlestoredb) (2.31.0)\n",
            "Collecting sqlparams (from singlestoredb)\n",
            "  Downloading sqlparams-5.1.0-py3-none-any.whl (16 kB)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.10/dist-packages (from singlestoredb) (0.41.2)\n",
            "Requirement already satisfied: packaging>=19.0 in /usr/local/lib/python3.10/dist-packages (from build->singlestoredb) (23.2)\n",
            "Requirement already satisfied: pyproject_hooks in /usr/local/lib/python3.10/dist-packages (from build->singlestoredb) (1.0.0)\n",
            "Requirement already satisfied: tomli>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from build->singlestoredb) (2.0.1)\n",
            "Requirement already satisfied: regex>=2022.3.15 in /usr/local/lib/python3.10/dist-packages (from parsimonious->singlestoredb) (2023.6.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->singlestoredb) (3.3.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->singlestoredb) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->singlestoredb) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->singlestoredb) (2023.7.22)\n",
            "Installing collected packages: sqlparams, parsimonious, singlestoredb\n",
            "Successfully installed parsimonious-0.10.0 singlestoredb-0.9.3 sqlparams-5.1.0\n"
          ]
        }
      ],
      "source": [
        "!pip install openai langchain tiktoken\n",
        "!pip install \"unstructured[slack]\" unstructured_inference\n",
        "!pip install singlestoredb"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IQ7WHhodCY2m"
      },
      "source": [
        "### Step 2: Configuration and API Initialization\n",
        "\n",
        "To interact with the OpenAI API and SingleStore database:\n",
        "\n",
        "1. Import the necessary modules.\n",
        "2. Set the OpenAI API key.\n",
        "3. Configure the SingleStore database connection details.\n",
        "\n",
        "**Note:** Ensure to keep your API keys confidential and avoid hardcoding them directly into scripts or notebooks.    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "chwdbhjbMaTM",
        "ExecuteTime": {
          "end_time": "2023-09-06T23:57:52.663280Z",
          "start_time": "2023-09-06T23:57:52.652220Z"
        }
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import openai\n",
        "\n",
        "openai.api_key = os.environ[\"OPENAI_KEY\"]\n",
        "os.environ[\"SINGLESTOREDB_URL\"] = \"<REPLACE SINGLESTOREDB URL>\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "91MEwPVXCY2m"
      },
      "source": [
        "\n",
        "### Step 3: Importing Modules\n",
        "\n",
        "These modules provide functionalities ranging from text retrieval to embeddings generation:\n",
        "\n",
        "- `RetrievalQA`: Used for retrieving answers to questions based on a dataset.\n",
        "- `load_qa_chain`: A function to load specific question-answering chains.\n",
        "- `TextLoader`: Helps in loading documents or texts.\n",
        "- `OpenAIEmbeddings`: Generates embeddings using OpenAI models.\n",
        "- `OpenAI`: Pertains to OpenAI's functionalities within the `langchain` library.\n",
        "- `PromptTemplate`: Useful for creating structured prompts for language models.\n",
        "- `CharacterTextSplitter`: Splits texts based on characters.\n",
        "- `SingleStoreDB`: Enables interactions with SingleStore databases.\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qXHn-zdqL_mH",
        "ExecuteTime": {
          "end_time": "2023-09-06T23:53:52.718366Z",
          "start_time": "2023-09-06T23:53:51.945919Z"
        }
      },
      "outputs": [],
      "source": [
        "from langchain.chains import RetrievalQA\n",
        "from langchain.chains.question_answering import load_qa_chain\n",
        "from langchain.document_loaders import TextLoader\n",
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "from langchain.llms import OpenAI\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain.vectorstores import SingleStoreDB"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Js_a4qO2CY2n"
      },
      "source": [
        "\n",
        "### Step 4: Data Ingestion from Unstructured's Slack Connector\n",
        "\n",
        "In this step, the goal is to fetch data from a Slack channel. The `unstructured-ingest` command allows for the ingestion of unstructured data from various sources, in this case, Slack:\n",
        "\n",
        "- The specific Slack channel ID and token are used to fetch data.\n",
        "- The data is stored locally in specified directories for download and structured output.\n",
        "- \"--partition-by-api\" to run this process using Unstructured's hosted API\n",
        "- An API key must be provided when using the hosted API\n",
        "\n",
        "Executing this command will result in the Slack channel's data being ingested and ready for further processing.\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-09-07T00:03:10.300279Z",
          "start_time": "2023-09-07T00:03:01.129277Z"
        },
        "id": "GODGVgOGCY2n",
        "outputId": "48d9cc52-6062-4d79-e5cf-f1a389ab2363"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/ronh/.pyenv/versions/3.10.12/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py:65: UserWarning: Specified provider 'TensorrtExecutionProvider' is not in available provider names.Available providers: 'CPUExecutionProvider'\n",
            "  warnings.warn(\n",
            "/Users/ronh/.pyenv/versions/3.10.12/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py:65: UserWarning: Specified provider 'CUDAExecutionProvider' is not in available provider names.Available providers: 'CPUExecutionProvider'\n",
            "  warnings.warn(\n",
            "2023-09-06 17:03:03,993 MainProcess INFO     Processing 1 docs\n",
            "2023-09-06 17:03:06,489 SpawnPoolWorker-2 DEBUG    File exists: slack-ingest-download/C044N0YV08G.xml, skipping get_file\n",
            "2023-09-06 17:03:06,489 SpawnPoolWorker-2 INFO     Processing slack-ingest-download/C044N0YV08G.xml\n",
            "2023-09-06 17:03:06,489 SpawnPoolWorker-2 DEBUG    Using remote partition (https://api.unstructured.io/general/v0/general)\n",
            "2023-09-06 17:03:09,851 SpawnPoolWorker-2 INFO     Wrote slack-ingest-output/C044N0YV08G.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Command executed successfully. Output:\n"
          ]
        }
      ],
      "source": [
        "import subprocess\n",
        "\n",
        "# partition-by-api\n",
        "command = [\n",
        "  \"unstructured-ingest\",\n",
        "    \"slack\",\n",
        "    \"--channels\", \"<REPLACE CHANNEL URL>\",\n",
        "    \"--token\", \"<REPLACE SLACK TOKEN>\",\n",
        "    \"--download-dir\", \"slack-ingest-download\",\n",
        "    \"--structured-output-dir\", \"slack-ingest-output\",\n",
        "    \"--partition-by-api\",\n",
        "    \"--api-key\", \"<REPLACE UNSTRUCTURED API KEY>\",\n",
        "    \"--reprocess\", \"--preserve-downloads\"\n",
        "]\n",
        "\n",
        "# Run the command\n",
        "process = subprocess.Popen(command, stdout=subprocess.PIPE)\n",
        "output, error = process.communicate()\n",
        "\n",
        "# Print output\n",
        "if process.returncode == 0:\n",
        "    print('Command executed successfully. Output:')\n",
        "    print(output.decode())\n",
        "else:\n",
        "    print('Command failed. Error:')\n",
        "    print(error.decode())"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 5: Loading Ingested Text Data\n",
        "\n",
        "After ingesting data from Slack:\n",
        "\n",
        "1. A `TextLoader` object loads the text data saved in the previous step.\n",
        "2. The `load()` method reads the content, making it available for further processing in the notebook.    "
      ],
      "metadata": {
        "collapsed": false,
        "id": "4m-oZZtwCY2o"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MwVOh_rYMFFN",
        "ExecuteTime": {
          "end_time": "2023-09-07T00:03:18.390665Z",
          "start_time": "2023-09-07T00:03:18.384573Z"
        }
      },
      "outputs": [],
      "source": [
        "loader = TextLoader(\"slack-ingest-download/C044N0YV08G.txt\")\n",
        "documents = loader.load()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 6: Splitting Text into Manageable Chunks\n",
        "\n",
        "Given the potentially large volume of ingested data, it's beneficial to split texts into smaller chunks:\n",
        "\n",
        "- The `CharacterTextSplitter` utility breaks down large text documents into chunks based on a specified character count.\n",
        "- This facilitates more efficient processing in later stages, especially when generating embeddings or performing retrievals.    "
      ],
      "metadata": {
        "collapsed": false,
        "id": "8LS1SuhBCY2o"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-09-07T00:03:20.596888Z",
          "start_time": "2023-09-07T00:03:20.584085Z"
        },
        "id": "ycszFgItCY2o",
        "outputId": "3e20f074-60f6-4fda-dab6-f81e91da8458"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Created a chunk of size 1316, which is longer than the specified 1000\n",
            "Created a chunk of size 1405, which is longer than the specified 1000\n",
            "Created a chunk of size 1054, which is longer than the specified 1000\n",
            "Created a chunk of size 1418, which is longer than the specified 1000\n",
            "Created a chunk of size 2421, which is longer than the specified 1000\n",
            "Created a chunk of size 1156, which is longer than the specified 1000\n",
            "Created a chunk of size 1398, which is longer than the specified 1000\n",
            "Created a chunk of size 3522, which is longer than the specified 1000\n",
            "Created a chunk of size 1212, which is longer than the specified 1000\n",
            "Created a chunk of size 3927, which is longer than the specified 1000\n",
            "Created a chunk of size 4182, which is longer than the specified 1000\n",
            "Created a chunk of size 1735, which is longer than the specified 1000\n",
            "Created a chunk of size 1459, which is longer than the specified 1000\n",
            "Created a chunk of size 1396, which is longer than the specified 1000\n",
            "Created a chunk of size 2692, which is longer than the specified 1000\n",
            "Created a chunk of size 1254, which is longer than the specified 1000\n",
            "Created a chunk of size 1151, which is longer than the specified 1000\n",
            "Created a chunk of size 2010, which is longer than the specified 1000\n",
            "Created a chunk of size 1399, which is longer than the specified 1000\n",
            "Created a chunk of size 1647, which is longer than the specified 1000\n",
            "Created a chunk of size 2004, which is longer than the specified 1000\n",
            "Created a chunk of size 1014, which is longer than the specified 1000\n",
            "Created a chunk of size 4529, which is longer than the specified 1000\n",
            "Created a chunk of size 1272, which is longer than the specified 1000\n",
            "Created a chunk of size 7212, which is longer than the specified 1000\n",
            "Created a chunk of size 1802, which is longer than the specified 1000\n",
            "Created a chunk of size 1038, which is longer than the specified 1000\n",
            "Created a chunk of size 3754, which is longer than the specified 1000\n",
            "Created a chunk of size 1112, which is longer than the specified 1000\n",
            "Created a chunk of size 1398, which is longer than the specified 1000\n"
          ]
        }
      ],
      "source": [
        "text_splitter = CharacterTextSplitter(chunk_size=1000,\n",
        "                                      chunk_overlap=100)\n",
        "\n",
        "texts = text_splitter.split_documents(documents)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 7: Setting Up Embeddings\n",
        "\n",
        "Embeddings are dense vector representations of text data. They play a crucial role in information retrieval systems:\n",
        "\n",
        "- The `OpenAIEmbeddings` class is used here, suggesting that embeddings are generated using models from OpenAI.\n",
        "- These embeddings will be used later for storing text chunks in the database and for efficient retrieval."
      ],
      "metadata": {
        "collapsed": false,
        "id": "-FNjWicWCY2p"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-09-07T00:03:21.734599Z",
          "start_time": "2023-09-07T00:03:21.732628Z"
        },
        "id": "YWmJLkvgCY2p"
      },
      "outputs": [],
      "source": [
        "embeddings = OpenAIEmbeddings()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 8: Storing Text in SingleStoreDB\n",
        "\n",
        "To enable efficient retrieval of information, the processed text chunks, along with their embeddings, are stored in a SingleStore database:\n",
        "\n",
        "- The `SingleStoreDB.from_documents` method is employed to achieve this. It takes in the text chunks, their corresponding embeddings, and stores them in a specified database table.\n",
        "- This setup ensures that when queries are posed later, the system can quickly retrieve relevant text chunks based on their embeddings."
      ],
      "metadata": {
        "collapsed": false,
        "id": "r3suoiBTCY2p"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-09-07T00:03:31.567491Z",
          "start_time": "2023-09-07T00:03:23.815990Z"
        },
        "id": "lhEsL2NwCY2p"
      },
      "outputs": [],
      "source": [
        "docsearch = SingleStoreDB.from_documents(texts,\n",
        "                                         embeddings,\n",
        "                                         table_name = \"pdf_docs3\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 9: Setting Up RetrievalQA with a Prompt Template\n",
        "\n",
        "The RetrievalQA model gets configured with the previously defined prompt template:\n",
        "\n",
        "1. The prompt template (`PROMPT`) is specified as a keyword argument.\n",
        "2. The `RetrievalQA.from_chain_type` method is used to set up the model with the \"stuff\" chain type, using the specified OpenAI model and the retriever object.\n",
        "\n",
        "This configuration ensures that queries to the model are structured according to the defined template, enhancing clarity and consistency in interactions."
      ],
      "metadata": {
        "collapsed": false,
        "id": "9bpU6zK2CY2p"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-09-07T00:03:31.581443Z",
          "start_time": "2023-09-07T00:03:31.567723Z"
        },
        "id": "5Jg57dnSCY2p"
      },
      "outputs": [],
      "source": [
        "# use prompt template\n",
        "prompt_template = \"\"\"\n",
        "Use the following pieces of context to answer the question at the end. If you're not sure, just say so.\n",
        "If there are potential multiple answers, summarize them as possible answers.\n",
        "\n",
        "{context}\n",
        "\n",
        "Question: {question}\n",
        "Answer:\n",
        "\"\"\"\n",
        "\n",
        "PROMPT = PromptTemplate(template=prompt_template,\n",
        "                        input_variables=[\"context\", \"question\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-09-07T00:03:31.581881Z",
          "start_time": "2023-09-07T00:03:31.572978Z"
        },
        "id": "yhLiBDR3CY2p",
        "outputId": "85a30c09-72c2-4af7-bcd2-4648864b39ca"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/ronh/.pyenv/versions/unstructured-3.10/lib/python3.10/site-packages/langchain/llms/openai.py:201: UserWarning: You are trying to use a chat model. This way of initializing it is no longer supported. Instead, please use: `from langchain.chat_models import ChatOpenAI`\n",
            "  warnings.warn(\n",
            "/Users/ronh/.pyenv/versions/unstructured-3.10/lib/python3.10/site-packages/langchain/llms/openai.py:786: UserWarning: You are trying to use a chat model. This way of initializing it is no longer supported. Instead, please use: `from langchain.chat_models import ChatOpenAI`\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# use prompt template\n",
        "qa_chain = load_qa_chain(OpenAI(), chain_type=\"stuff\")\n",
        "chain_type_kwargs = {\"prompt\": PROMPT}\n",
        "\n",
        "qa = RetrievalQA.from_chain_type(llm=OpenAI(model_name='gpt-4-0613'),\n",
        "                                 chain_type=\"stuff\",\n",
        "                                 retriever=docsearch.as_retriever(),\n",
        "                                 chain_type_kwargs=chain_type_kwargs)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 10: Query about a Specific Task\n",
        "\n",
        "To address distinct tasks or challenges:\n",
        "\n",
        "1. A query string is set up, asking a specific question.\n",
        "2. The `run` method of the RetrievalQA model is invoked with the query.\n",
        "\n",
        "This demonstrates the capability of the RetrievalQA model to handle a wide array of queries and provide relevant responses based on the ingested data."
      ],
      "metadata": {
        "collapsed": false,
        "id": "8nv_ADkQCY2p"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-09-07T00:03:45.276079Z",
          "start_time": "2023-09-07T00:03:31.578400Z"
        },
        "id": "yPP1xbIiCY2p",
        "outputId": "6116c93c-b55f-4218-bd90-d80c5c90b4ca"
      },
      "outputs": [
        {
          "data": {
            "text/plain": "'From the context given, here are the steps for extracting tables from a PDF file:\\n\\n1. Inferring table structure is off by default, and for now only works on PDFs. Use it by calling `partition(\"yourfile.pdf\", pdf_infer_table_structure=True)`, or `partition_pdf(\"yourfile.pdf\", infer_table_structure=True)`.\\n2. The extraction works best when the OCR for the table cells is done by PaddleOCR. \\n3. The table structure is stored in the element metadata, under the `text_as_html` property, in HTML form.\\n\\nPlease note that the platform is currently working on resolving some known issues on table extraction, such as:\\n- Reports of some cases where the text from cells is duplicated in non-table elements.\\n- Reports of the table structure not being properly captured when the table spans multiple pages.'"
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "query = \"How can I extract table from a PDF file?\"\n",
        "qa.run(query)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-09-07T00:03:52.640140Z",
          "start_time": "2023-09-07T00:03:45.253510Z"
        },
        "id": "e9oUu_YtCY2p",
        "outputId": "e5e0cccd-e13c-4737-bbac-79091d6dcb0f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": "'Yes, you can use your own model for inferences. However, it would need to be wrapped in the `UnstructuredObjectDetectionModel` class to ensure the inputs and outputs align for the downstream functionality. You may follow the interface defined in the Chipper model implementation or refer to the documentation in the unstructured-inference repository for more precise information.'"
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "query = \"Can I bring my own model for inferences?\"\n",
        "qa.run(query)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-09-07T00:03:58.253923Z",
          "start_time": "2023-09-07T00:03:52.629082Z"
        },
        "id": "w_rsZVTVCY2q",
        "outputId": "720928e3-c1e3-4ea6-9d78-b1735963fbc1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": "\"The Unstructured tool chunks texts by grabbing text of the same type in roughly paragraph sized chunks. The specific chunking logic depends on the document type and it's more successful with chunking some document types than others.\""
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "query = \"How does Unstructured tool chunk texts?\"\n",
        "qa.run(query)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-09-07T00:04:08.018280Z",
          "start_time": "2023-09-07T00:03:58.232453Z"
        },
        "id": "rsstdN_FCY2q",
        "outputId": "282ee889-28a9-4daa-c615-5cc908a18ad9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": "'There are not many quick solutions for this, but one strategy suggested is to process individual pages in parallel. However, there are limits to speed improvement when working on a CPU. It seems the support is working on improving the speed. Also, depending on the parameters you use, like `strategy=\"auto\"`, and the value of `pdf_infer_table_structure`, speed can vary. Changes to `strategy` might result in different PDF elements being shown. Merging related sequential elements at `partition` time is also considered to speed up the process, but it is not implemented yet.'"
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "query = \"How can I speed up the execution of PDF parsing?\"\n",
        "qa.run(query)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-09-07T00:04:24.036480Z",
          "start_time": "2023-09-07T00:04:07.998277Z"
        },
        "id": "YQSaZNweCY2q",
        "outputId": "e254b02c-184d-4747-d856-9a65ad5abb1e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": "\"The user wants to extract image URLs from the HTML content of news articles. They've explored the source code and come across a potential method but are unclear on how to utilize it. There are several proposed solutions to this issue:\\n\\n1. The first solution was to use a cleaning 'brick' or module on the result of the partition function to extract the URL. However, the user was confused by this solution, as the cleaner module indicated was for email extraction, not image URL extraction.\\n\\n2. The user was later informed that there isn't a defined pattern for catching image URLs from HTML content within the codebase. An issue was created for this and a solution was being devised, involving a pattern similar to other solutions.\\n\\n3. Another solution was to extract the URLs from the JSON response structure from the partition response. There is no clear consensus or final solution provided in the context given. The engineering team will be working on a more generic extraction brick/module to handle this type of URL structures, trying to combine both regex and additional logic.\""
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "query = \"How to extract image URLs from HTML content?\"\n",
        "qa.run(query)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "start_time": "2023-09-06T23:54:06.415262Z"
        },
        "id": "AVWbe5xoCY2q"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "unstructured-3.10.12",
      "language": "python",
      "name": "unstructured-3.10.12"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}